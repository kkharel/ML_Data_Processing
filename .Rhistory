return ci_A, ci_B
confidence_interval_plot(data.control[1]/data.control[0], data.treatment[1]/data.treatment[0], data.control[0], data.treatment[0], alpha = 0.05)
ci_A, ci_B = confidence_interval_plot(data['control'][0] / data['control'][1], data['treatment'][0] / data['treatment'][1], data['control'][1], data['treatment'][1], alpha=0.05)
print("Confidence Interval for Variant A:", ci_A)
print("Confidence Interval for Variant B:", ci_B)
ci_A, ci_B = confidence_interval_plot(data.control[1]/data.control[0], data.treatment[1]/data.treatment[0], data.control[0], data.treatment[0], alpha = 0.05)
print("Confidence Interval for Variant A:", ci_A)
print("Confidence Interval for Variant B:", ci_B)
import statsmodels.stats.power as smp
baseline = 0.1
delta = 0.03
power = 0.8
sig_level = 0.05
result = smp.zt_ind_solve_power(effect_size=delta / baseline,
nobs1=None,
alpha=sig_level,
power=power,
ratio=1.0,
alternative='two-sided')
print(result)
result = smp.zt_ind_solve_power(effect_size=delta / baseline,
nobs1=None,
alpha=sig_level,
power=power,
alternative='two-sided')
print(result)
baseline = 0.1
delta = 0.03
power = 0.2
sig_level = 0.05
result = smp.zt_ind_solve_power(effect_size=delta / baseline,
nobs1=None,
alpha=sig_level,
power=power,
alternative='two-sided')
print(result)
baseline = 0.1
delta = 0.03
power = 0.8
sig_level = 0.05
result = smp.zt_ind_solve_power(effect_size=delta / baseline,
nobs1=None,
alpha=sig_level,
power=power,
alternative='two-sided')
print(result)
def calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='one_sided'):
if mode not in ['one_sided', 'two_sided']:
raise ValueError('The modes are one_sided and two_sided')
Z_alpha_over_2 = sp_stats.norm.ppf(1 - alpha / 2)
Z_beta = sp_stats.norm.ppf(1 - beta)
pooled_variance = p_control * (1 - p_control)
if mode == 'two_sided':
n = (2 * (Z_alpha_over_2 + Z_beta)**2 * pooled_variance) / MDE**2
else:
n = (Z_alpha_over_2**2 * pooled_variance) / MDE**2
return int(np.ceil(n))
MDE = 0.03  # Minimum Detectable Effect (3% improvement)
alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
MDE = 0.01  # Minimum Detectable Effect (3% improvement)
alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
MDE = 0.02  # Minimum Detectable Effect (3% improvement)
alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
def calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='one_sided'):
if mode not in ['one_sided', 'two_sided']:
raise ValueError('The modes are one_sided and two_sided')
Z_alpha_over_2 = sp_stats.norm.ppf(1 - alpha / 2)
Z_beta = sp_stats.norm.ppf(1 - beta)
pooled_variance = p_control * (1 - p_control)
if mode == 'two_sided':
n = (2 * (Z_alpha_over_2 + Z_beta)**2 * pooled_variance) / MDE**2
else:
n = (Z_alpha_over_2**2 * pooled_variance) / MDE**2
return int(np.ceil(n))
MDE = 0.02  # Minimum Detectable Effect (3% improvement)
alpha = 0.05  # Significance level (5%)
beta = 0.2  # Power (80%)
p_control = 0.10  # Estimated proportion in the control group (10%)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
sample_data = generate_sample_data(required_sample_size = sample, p_control, MDE)
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
samples = calculate_sample_size_proportions(MDE, alpha, beta, p_control, mode='two_sided')
samples
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
samples
num_users = int(required_sample_size * 2)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
print(aggregated_data)
aggregated_data.T
num_users = int(required_sample_size)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
sample_data
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
samples
num_users = int(required_sample_size)
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) = conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(required_sample_size)
num_users
num_users = int(samples)
num_users
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) == conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
# Data Generation Sanity Check
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
unique_dates = sample_data['timestamp'].dt.date.nunique()
unique_dates
print(sample_data)
aggregated_data = sample_data.groupby('group').agg(
trials=('user_id', lambda x: x.nunique()),
successes=('converted', 'sum'),
)
# Display aggregated data
print(aggregated_data)
aggregated_data.T
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
group_assignments# Generate conversions
num_users = int(samples*2)
num_users
# Set random seed for reproducibility
np.random.seed(42)
# Generate user IDs
user_ids = np.arange(1, num_users + 1)
# Generate timestamps
start_date = pd.to_datetime('2023-08-18')
end_date = start_date + pd.DateOffset(days=29)
timestamps = pd.date_range(start=start_date, end=end_date, freq='H').tolist()
timestamps = np.random.choice(timestamps, size=num_users, replace=True)
# Generate group assignments
group_assignments = np.random.choice(['control', 'treatment'], size=num_users, p=[0.5, 0.5])
# Generate conversions
conversion_rates = {
'control': p_control,
'treatment': p_control + MDE
}
# Reset seed for conversions
np.random.seed(42)
conversion_probabilities = np.where(group_assignments == 'control', conversion_rates['control'], conversion_rates['treatment'])
conversion_status = np.random.rand(num_users) <= conversion_probabilities
# Create a DataFrame
sample_data = pd.DataFrame({
'user_id': user_ids,
'timestamp': timestamps,
'group': group_assignments,
'converted': conversion_status.astype(int)
})
print(sample_data)
control_group_data = sample_data[sample_data['group'] == 'control']
control_group_data['converted'].mean()
treatment_group_data = sample_data[sample_data['group'] == 'treatment']
treatment_group_data['converted'].mean()
control_count = sample_data[sample_data['group'] == 'control'].shape[0]
control_count
treatment_count = sample_data[sample_data['group'] == 'treatment'].shape[0]
treatment_count
reticulate::repl_python()
import sympy
from sympy import *
from sympy.plotting import plot
plot(-(4/5)*x + 7)
x = Symbol("x")
plot(-(4/5)*x + 7)
def is_perpendicular(m1,m2):
if (m1*m2+1) == 0:
return True
else:
return False
is_perpendicular(-2, 1/2)
is_perpendicular(-2, 2)
reticulate::repl_python()
import pandas as pd
import random
import string
def generate_random_data(num_records=10):
data = {
'first_name': [random.choice(['Alice', 'Bob', 'Charlie', 'David', 'Eva']) for _ in range(num_records)],
'last_name': [random.choice(['Smith', 'Johnson', 'Williams', 'Jones', 'Brown']) for _ in range(num_records)],
'preferred_contact': [random.choice(['Email', 'Phone', 'SMS']) for _ in range(num_records)],
'last_four_digits': [''.join(random.choices(string.digits, k=4)) for _ in range(num_records)],
'billing_zip_code': [''.join(random.choices(string.digits, k=5)) for _ in range(num_records)],
'non_mfa_ip_addresses': [f'{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}' for _ in range(num_records)]
}
return pd.DataFrame(data)
num_records_youtube = 100
df_youtube = generate_random_data(num_records_youtube)
num_records_spotify = 100
df_spotify = generate_random_data(num_records_spotify)
youtube_records_to_include = random.sample(range(len(df_youtube)), num_records_spotify)
for index in youtube_records_to_include:
df_spotify.loc[len(df_spotify)] = df_youtube.loc[index]
print("df_youtube:")
print(df_youtube)
print("\ndf_spotify:")
print(df_spotify)
import os
os.cwd("C:\Users\kkhar\ML_Data_Processing")
os.chdir("C:\Users\kkhar\ML_Data_Processing")
os.chdir("C:/Users/kkhar/ML_Data_Processing")
def generate_random_data(num_records=10):
data = {
'first_name': [random.choice(['Alice', 'Bob', 'Charlie', 'David', 'Eva']) for _ in range(num_records)],
'last_name': [random.choice(['Smith', 'Johnson', 'Williams', 'Jones', 'Brown']) for _ in range(num_records)],
'preferred_contact': [random.choice(['Email', 'Phone', 'SMS']) for _ in range(num_records)],
'last_four_digits': [''.join(random.choices(string.digits, k=4)) for _ in range(num_records)],
'billing_zip_code': [''.join(random.choices(string.digits, k=5)) for _ in range(num_records)],
'non_mfa_ip_addresses': [f'{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}.{random.randint(0, 255)}' for _ in range(num_records)]
}
return pd.DataFrame(data)
num_records_youtube = 100
df_youtube = generate_random_data(num_records_youtube)
num_records_spotify = 100
df_spotify = generate_random_data(num_records_spotify)
youtube_records_to_include = random.sample(range(len(df_youtube)), num_records_spotify)
for index in youtube_records_to_include:
df_spotify.loc[len(df_spotify)] = df_youtube.loc[index]
print("df_youtube:")
print(df_youtube)
print("\ndf_spotify:")
print(df_spotify)
